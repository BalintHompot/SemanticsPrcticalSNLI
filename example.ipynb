{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining sentence representations on SNLI\n",
    "\n",
    "This notebook gives an example on how to use the provided code. It also contains examples on how to customize runs, and the analysis is also included here at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the functions used for training and evaluation from the other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import *\n",
    "from trainFunctions import *\n",
    "from utils import *\n",
    "from sentEval import runSentEval\n",
    "\n",
    "## let's ignore the pytorch warnings for readability\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the SNLI data and the field which contains the preprocessing pipeline and metadata (this takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, TEXT, LABEL = get_data()\n",
    "data = {\"train\":train_data, \"val\": val_data, \"test\": test_data}\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the parameters that are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"vector_size\" : 300,\n",
    "    \"vocab_size\" : len(TEXT.vocab),\n",
    "    \"pretrained\" : TEXT.vocab.vectors,\n",
    "    \"pad_idx\" : TEXT.vocab.stoi[TEXT.pad_token]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define default parameters that are used during the sweeping \\\n",
    "(At a time we only sweep one parameters while the rest is unchanged, this is for saving time, however may not give the best results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change default parameters\n",
    "default_params = {\n",
    "    \"lr_decrease_factor\":5,\n",
    "    \"lr_stopping\" : 1e-6,\n",
    "    \"layer_num\" : 1,\n",
    "    \"layer_size\" : 512,\n",
    "    \"lr\" : 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the ranges in which these are sweeped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change parameters ranges\n",
    "param_ranges = {\n",
    "    \"learning rates\":[0.01, 0.001],\n",
    "    \"lr_decrease_factors\":[3, 5],\n",
    "    \"lr_stoppings\": [1e-5, 1e-6], \n",
    "    \"layer nums\":[1,2],\n",
    "    \"layer sizes\":[512,1024],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in all previous dictionaries the keys are fixed and the models are looking for them. Only change the values in them if you want to try different setups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the list of encoder models that we want to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [MeanEncoder,LSTMEncoder,BiLSTMEncoder, MaxBiLSTMEncoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loop through the encoders and perform\n",
    "* parameter search\n",
    "* constructing a model with the best parameters and train it\n",
    "* test the model\n",
    "* store the trained model and the dev/test results\n",
    "* evaluate on SentEval\n",
    "\n",
    "For each of these tasks there is a function, see readme for more details\n",
    "\n",
    "(Note: I wouldn't recommend actually running it, it takes very long. All the cells below it will work as the outputs are stored)\n",
    "\n",
    "(Note2: we use the default \"best\" runName here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoderClass in encoders:\n",
    "    # searching for best params\n",
    "    best_params_for_model = paramSweep(encoderClass, data, default_params, param_ranges, metadata, forceOptimize = False)\n",
    "    # training model with best params (and saving training plots)\n",
    "    best_model = construct_and_train_model_with_config(encoderClass, data, best_params_for_model, metadata, forceRetrain=False)\n",
    "    # testing the best model\n",
    "    best_model_results = testModel(best_model, data)\n",
    "    # saving best model and results\n",
    "    save_model_and_res(best_model, best_model_results)\n",
    "    # running SentEval for the model\n",
    "    runSentEval(best_model, TEXT, tasks=\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. If the above cell is finished (it may take days, depending on the ranges), all trained models and their configs and results are stored in the appropriately named folders.\n",
    "\n",
    "We can test some examples, just pass an encoder name, and the text field (for preprocessing) and label field (for getting the label, if the fields are not passed they are loaded by the script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testExample(\"Pooled BiLSTM\", TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Don't forget to exit above!)\n",
    "\n",
    "To more formally assess the performance, we can create tables with results, similarly to the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderNames = [\"Vector mean\", \"LSTM\", \"BiLSTM\", \"Pooled BiLSTM\"]  ### you could select a subset, or store the name in the above loop as well\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printResults(encoderNames, resultType = \"SentEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default parameters and ranges\n",
    "The default parameters and the ranges can be changed by defining different ones in the dictionaries given as input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### I just want to train one model with specified params\n",
    "You can always call the above functions separately, just make sure you define valid inputs (note that the keys are not named the same as in the config), and give a **run name**. \n",
    "\n",
    "The run name will define in what directories will the output be saved. It defaults to \"best\", so on default the ouputs are saved in runs/best/ best_configs, best_models, best_model_results, but given e.g. \"lstm\" they would be saved to runs/lstm/lstm_configs ... (directories created in the script). Any function that needs to access some stored file can take runName as argument, and all default to \"best\". As an example, training a simple LSTM encoder without sweeping and SentEval evaluation, with custom params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"learning rate\": 0.0001,\n",
    "    \"lr_stopping\": 1e-06,\n",
    "    \"lr_decrease_factor\": 7,\n",
    "    \"number of layers\": 1,\n",
    "    \"number of neurons per layer\": 256\n",
    "}\n",
    "\n",
    "runName = \"custom_lstm_run\"\n",
    "\n",
    "trained_model = construct_and_train_model_with_config(MeanEncoder, data, custom_params, metadata, runName=runName)\n",
    "trained_model_results = testModel(trained_model, data)\n",
    "save_model_and_res(trained_model, trained_model_results, runName = runName)\n",
    "\n",
    "## we can also call the result printing with the runName\n",
    "printResults([\"Vector mean\"], resultType = \"SNLI\", runName = runName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have the best configs stored, but I want to rerun sweeping\n",
    "Set forceOptimize=True in paramSweep, and the script ignores the stored best config and overwrites it. Example with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_for_model = paramSweep(LSTMEncoder, data, default_params, param_ranges, metadata, forceOptimize = True, runName = \"retrain_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have the best model stored, but I have changed the best params\n",
    "Set forceRetrain=True in construct_and_train_model_with_config so it ignores the stored model and overwrites with a new one. Example with LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = construct_and_train_model_with_config(LSTMEncoder, data, custom_params, metadata, forceRetrain=True, runName = \"retrain_test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and models that are shown above (so the ones under the \"best\" folders) are actually not the output of parameter sweeping, but using the same parameters as in the Conneau paper for easier comparison.\\\n",
    "\\\n",
    "If you are familiar with the paper, you can see that the SNLI results are comparable, somwhat even better than the ones reported there, with showing the same pattern: the baseling vector mean approach is the worse, LSTM is noticably better, BiLSTM is slightly better than LSTM, and the pooled BiLSTM performs the best. This is as expected since:\n",
    "* The vector mean approach is a very naive compositional approach: as it only averages, it contains no information about word order, the single Glove vectors contain no information about context, the the model does not employ any attention mechanism to focus on the important parts.\n",
    "* The LSTM method is sequential in nature, and the running cell state with input and forget gates offers a mechanism that could encapsulate some contextual meaning from the word vectors. The direction is still strictly unidirectional and the LSTM still has trouble seeing long distance relations (though better than RNN) as processes words one by one. The fact that we only use the last hidden state makes it hard to see the words at the beginning, or separate word's contributions in general.\n",
    "* The BiLSTM improves on the previous one by concatenating two unidirectional approaches, one from the end. This introduces a shallow bidirectionality where the information from the other end is also encoded (however does not solve the problem that every other word should be seen at the same as in transformers).\n",
    "* The pooled BiLSTM works best, as it adds a weak form of attention to the model. Though not queried based on the output, the fact that every word has a chance of contribution to the final output makes it possible for the model to extract more meaningful representation regardless the position.\n",
    "\n",
    "However, if you've read the original paper, you might have noticed that, while the SNLI performance is good, the performance on the transfer tasks is quite bad, worse than the reported ones in the paper (and actually worse than the baseline model's)\n",
    "\n",
    "To investigate this, we should note that there are two differences from the original paper's setup:\n",
    "* I used **dropout** of 0.5 at both the encoder and the classifier (original did not report any dropout)\n",
    "* I used **Adam optimizer**. For that, the (starting) learning rate and the stopping had to be reduced, to 0.001 and 1e-06.\n",
    "\n",
    "This gives the idea that we are not overfitting the data, but we **are overfitting the task**. As the paper mentions this is probably due to the fact that we are using Adam which gives generally better fit on the objective task. To test that I ran a version with SGD optimizer (change optimizer in the trainFunctions), the corresponding files can be found in the runs/sgd/sgd_... folders.\n",
    "\n",
    "Let us look at how the results compare:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderNames = [\"LSTM\", \"BiLSTM\", \"Pooled BiLSTM\"]  \n",
    "print(\"\\nResults with Adam\\n\")\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\")\n",
    "print(\"\\nResults with SGD\\n\")\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\", runName = \"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is that the transfer performance is indeed noticably higher for the SGD version, supporting the idea that Adam is overfitting the task. To get a better picture let's look at the separate task results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nResults with Adam\\n\")\n",
    "printResults(encoderNames, resultType = \"SentEval\")\n",
    "print(\"\\nResults with SGD\\n\")\n",
    "printResults(encoderNames, resultType = \"SentEval\", runName=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that on task directly involving inference or sentence comparison (SickEntailment, STS14) we achieve high performance, whereas as we move further from the original task (MR, TREC) the performance goes down rapidly. This difference is bigger when using Adam than with SGD, further strengthening the idea that Adam makes the model overfit on the specific task more strongly than SGD, so for transfer use the latter is more applicable even though the original performance is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The idea of transfer learning for NLP has been highly influental and one of the main reasons that the field has been one of the fastest developing ones in the past years. However, this specific task does not prove to be \"NLP complete\" as even slightly complex models easily overfit on it without generalizing very well on other NLP tasks. The architectures presented here also lack some of the key aspects of the models that have proven to be the most effective in recent years, such as proper attention or deep bidirectionality. The upside of these models is that they are lightweight in comparison to transformers so it is possible to learn them from skratch. If that is not an important aspect, it is normally a better approach to use pre-trained transformer language model as both the pretraining task (masked language modelling) is more NLP-complete, and the deep attention based architecture is proven to represent richer linguistic content. Still, the main point of the paper and the experiments, that transfer learning should be exploited in NLP is proven, and has become a standard practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further questions\n",
    "* The finding is quite curious and it should be interesing to look into the theoretical background on why this happens. As mentioned in the paper Adam converges faster, but for a proper comparison the training should be stoppped at equal performance level so we know that if we see a difference, it's from the nature of the optimizer and not about how far they are into convergence.\n",
    "* When starting the training, the first versions used large batch size (300), but it was stopped after discussing with the TA-s that it would probably hurt the performance. The runs could not finish so only the results for the baseline and the LSTM model were obtained, but surprisingly on both the performance was better. It would be interesting to actually run the full experiment with large batch size to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('dl': conda)",
   "language": "python",
   "name": "python37364bitdlconda4a4146cd19534b21b826a4d8caea02c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
