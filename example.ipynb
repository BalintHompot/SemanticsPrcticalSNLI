{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining language models on SNLI\n",
    "\n",
    "This notebook gives an example on how to use the provided code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the functions used for training and evaluation from the other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import *\n",
    "from trainFunctions import *\n",
    "from utils import *\n",
    "from sentEval import runSentEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the SNLI data and the field which contains the preprocessing pipeline and metadata (this takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data, TEXT = get_data()\n",
    "data = {\"train\":train_data, \"val\": val_data, \"test\": test_data}\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the parameters that are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"vector_size\" : 300,\n",
    "    \"vocab_size\" : len(TEXT.vocab),\n",
    "    \"pretrained\" : TEXT.vocab.vectors,\n",
    "    \"pad_idx\" : TEXT.vocab.stoi[TEXT.pad_token]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define default parameters that are used during the sweeping \\\n",
    "(At a time we only sweep one parameters while the rest is unchanged, this is for saving time, however may not give the best results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change default parameters\n",
    "default_params = {\n",
    "    \"lr_decrease_factor\":5,\n",
    "    \"lr_stopping\" : 1e-6,\n",
    "    \"layer_num\" : 1,\n",
    "    \"layer_size\" : 512,\n",
    "    \"lr\" : 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the ranges in which these are sweeped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change parameters ranges\n",
    "param_ranges = {\n",
    "    \"learning rates\":[0.01, 0.001],\n",
    "    \"lr_decrease_factors\":[3, 5],\n",
    "    \"lr_stoppings\": [1e-5, 1e-6], \n",
    "    \"layer nums\":[1,2],\n",
    "    \"layer sizes\":[512,1024],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in all previous dictionaries the keys are fixed and the models are looking for them. Only change the values in them if you want to try different setups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the list of encoder models that we want to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [MeanEncoder,LSTMEncoder,BiLSTMEncoder, MaxBiLSTMEncoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loop through the encoders and perform\n",
    "* parameter search\n",
    "* constructing a model with the best parameters and train it\n",
    "* test the model\n",
    "* store the trained model and the dev/test results\n",
    "* evaluate on SentEval\n",
    "\n",
    "For each of these tasks there is a function, see readme for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoderClass in encoders:\n",
    "    # searching for best params\n",
    "    best_params_for_model = paramSweep(encoderClass, data, default_params, param_ranges, metadata, forceOptimize = False)\n",
    "    # training model with best params (and saving training plots)\n",
    "    best_model = construct_and_train_model_with_config(encoderClass, data, best_params_for_model, metadata, forceRetrain=False)\n",
    "    # testing the best model\n",
    "    best_model_results = testModel(best_model, data)\n",
    "    # saving best model and results\n",
    "    save_model_and_res(best_model, best_model_results)\n",
    "    # running SentEval for the model\n",
    "    runSentEval(best_model, TEXT, tasks=\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. If the above cell is finished (it may take days, depending on the ranges), all trained models and their configs and results are stored in the appropriately named folders. To create a comparison table we can call the printResults function with either SNLI or SentEval as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderNames = [\"Vector mean\", \"LSTM\", \"BiLSTM\", \"Pooled BiLSTM\"]  ### you could select a subset, or store the name in the above loop as well\n",
    "printResults(encoderNames, resultType = \"SNLI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printResults(encoderNames, resultType = \"SentEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default parameters and ranges\n",
    "The default parameters and the ranges can be changed by defining different \n",
    "\n",
    "#### I have the best configs stored, but I want to rerun sweeping\n",
    "Set forceOptimize=True in paramSweep, and the script ignores the stored best config and overwrites it\n",
    "\n",
    "#### I have the best model stored, but I have changed the best params\n",
    "Set forceRetrain=True in construct_and_train_model_with_config so it ignores the stored model and overwrites with a new one\n",
    "\n",
    "#### I just want to train one model with specified params\n",
    "You can always call the above functions separately, just make sure you define valid inputs (note that the keys are not named the same as in the config), and give a run name. The run name will define in what directories will the output be saved. It defaults to \"best\", so on default the ouput is saved in best_configs, best_models, best_model_results, but given e.g. \"lstm\" they would be saved to lstm_configs ... (directories created in the script). As an example, training a simple LSTM encoder without sweeping and SentEval evaluation, with custom params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"learning rate\": 0.0001,\n",
    "    \"lr_stopping\": 1e-06,\n",
    "    \"lr_decrease_factor\": 7,\n",
    "    \"number of layers\": 1,\n",
    "    \"number of neurons per layer\": 256\n",
    "}\n",
    "\n",
    "runName = \"custom_lstm_run\"\n",
    "\n",
    "trained_model = construct_and_train_model_with_config(LSTMEncoder, data, custom_params, metadata, runName=runName)\n",
    "trained_model_results = testModel(trained_model, data)\n",
    "save_model_and_res(trained_model, trained_model_results, runName = runName)\n",
    "\n",
    "## we can also call the result printing with the runName\n",
    "printResults([\"LSTM\"], resultType = \"SNLI\", runName = runName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and models that are reported in this notebook (so the ones under the \"best\" folders) are actually not the output of parameter sweeping, but using the same parameters as in the Conneau paper for easier comparison.\\\n",
    "\\\n",
    "There are two differences from the original paper's setup:\n",
    "* I used dropout of 0.5 at both the encoder and the classifier (original did not report any dropout)\n",
    "* I used Adam optimizer. For that, the (starting) learning rate and the stopping had to be reduced, to 0.001 and 1e-06.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('dl': conda)",
   "language": "python",
   "name": "python37364bitdlconda4a4146cd19534b21b826a4d8caea02c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
