{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining language models on SNLI\n",
    "\n",
    "This notebook gives an example on how to use the provided code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the functions used for training and evaluation from the other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoders import *\n",
    "from trainFunctions import *\n",
    "from utils import *\n",
    "from sentEval import runSentEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the SNLI data and the field which contains the preprocessing pipeline and metadata (this takes a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing raw input and preprocessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-17 14:44:57,730 : Loading vectors from .vector_cache/glove.840B.300d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Building vocabulary with GloVe\n",
      "done\n",
      "Loading data into iterables\n",
      "done, returning data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, TEXT, LABEL = get_data()\n",
    "data = {\"train\":train_data, \"val\": val_data, \"test\": test_data}\n",
    "\n",
    "print(\"Data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the parameters that are fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    \"vector_size\" : 300,\n",
    "    \"vocab_size\" : len(TEXT.vocab),\n",
    "    \"pretrained\" : TEXT.vocab.vectors,\n",
    "    \"pad_idx\" : TEXT.vocab.stoi[TEXT.pad_token]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define default parameters that are used during the sweeping \\\n",
    "(At a time we only sweep one parameters while the rest is unchanged, this is for saving time, however may not give the best results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change default parameters\n",
    "default_params = {\n",
    "    \"lr_decrease_factor\":5,\n",
    "    \"lr_stopping\" : 1e-6,\n",
    "    \"layer_num\" : 1,\n",
    "    \"layer_size\" : 512,\n",
    "    \"lr\" : 0.001,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the ranges in which these are sweeped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## edit this to change parameters ranges\n",
    "param_ranges = {\n",
    "    \"learning rates\":[0.01, 0.001],\n",
    "    \"lr_decrease_factors\":[3, 5],\n",
    "    \"lr_stoppings\": [1e-5, 1e-6], \n",
    "    \"layer nums\":[1,2],\n",
    "    \"layer sizes\":[512,1024],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in all previous dictionaries the keys are fixed and the models are looking for them. Only change the values in them if you want to try different setups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the list of encoder models that we want to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [MeanEncoder,LSTMEncoder,BiLSTMEncoder, MaxBiLSTMEncoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we loop through the encoders and perform\n",
    "* parameter search\n",
    "* constructing a model with the best parameters and train it\n",
    "* test the model\n",
    "* store the trained model and the dev/test results\n",
    "* evaluate on SentEval\n",
    "\n",
    "For each of these tasks there is a function, see readme for more details\n",
    "\n",
    "(Note: I wouldn't recommend actually running it, it takes very long. All the cells below it will work as the outputs are stored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoderClass in encoders:\n",
    "    # searching for best params\n",
    "    best_params_for_model = paramSweep(encoderClass, data, default_params, param_ranges, metadata, forceOptimize = False)\n",
    "    # training model with best params (and saving training plots)\n",
    "    best_model = construct_and_train_model_with_config(encoderClass, data, best_params_for_model, metadata, forceRetrain=False)\n",
    "    # testing the best model\n",
    "    best_model_results = testModel(best_model, data)\n",
    "    # saving best model and results\n",
    "    save_model_and_res(best_model, best_model_results)\n",
    "    # running SentEval for the model\n",
    "    runSentEval(best_model, TEXT, tasks=\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. If the above cell is finished (it may take days, depending on the ranges), all trained models and their configs and results are stored in the appropriately named folders.\n",
    "\n",
    "We can test some examples, just pass an encoder name, and the text field (for preprocessing) and label field (for getting the label, if the fields are not passed they are loaded by the script):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type a premise (x to exit): This is an example text\n",
      "Type a hypothesis (x to exit): This is not an example text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/balint/anaconda3/envs/dl/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict is: Neutral\n",
      "Type a premise (x to exit): This is an other example\n",
      "Type a hypothesis (x to exit): I like omletts\n",
      "Verdict is: Entailment\n",
      "Type a premise (x to exit): I like football\n",
      "Type a hypothesis (x to exit): The sun is great\n",
      "Verdict is: Contradiction\n",
      "Type a premise (x to exit): x\n"
     ]
    }
   ],
   "source": [
    "testExample(\"Vector mean\", TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To more formally assess the performance, we can create tables with results, similarly to the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         |   dev accuracy |   test accuracy:  |   transfer macro |   transfer micro |\n",
      "|---------------+----------------+-------------------+------------------+------------------|\n",
      "| Vector mean   |        73.3981 |           73.8839 |          80.5629 |          81.4773 |\n",
      "| LSTM          |        82.67   |           82.7618 |          78.0964 |          79.6539 |\n",
      "| BiLSTM        |        82.3872 |           82.4269 |          77.1967 |          79.0204 |\n",
      "| Pooled BiLSTM |        84.3397 |           84.3141 |          78.25   |          79.762  |\n"
     ]
    }
   ],
   "source": [
    "encoderNames = [\"Vector mean\", \"LSTM\", \"BiLSTM\", \"Pooled BiLSTM\"]  ### you could select a subset, or store the name in the above loop as well\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model         |    MR |    CR |   MPQA |   SUBJ |   SST2 |   TREC | MRPC        |   SICKEntailment | STS14     |\n",
      "|---------------+-------+-------+--------+--------+--------+--------+-------------+------------------+-----------|\n",
      "| Vector mean   | 74.33 | 78.01 |  84.6  |  89.53 |  79.24 |   80.8 | 71.83/81.31 |            77.43 | 0.5/0.52  |\n",
      "| LSTM          | 68.54 | 75.23 |  83.48 |  82.1  |  71.94 |   65.6 | 69.51/78.77 |            82.52 | 0.53/0.51 |\n",
      "| BiLSTM        | 68.19 | 75.1  |  83.67 |  82.54 |  70.02 |   66.2 | 70.78/80.75 |            82.06 | 0.54/0.51 |\n",
      "| Pooled BiLSTM | 73.21 | 80.05 |  85.26 |  88.57 |  77.38 |   81.6 | 72.75/81.18 |            83.8  | 0.64/0.61 |\n"
     ]
    }
   ],
   "source": [
    "printResults(encoderNames, resultType = \"SentEval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default parameters and ranges\n",
    "The default parameters and the ranges can be changed by defining different ones in the dictionaries given as input\n",
    "\n",
    "#### I have the best configs stored, but I want to rerun sweeping\n",
    "Set forceOptimize=True in paramSweep, and the script ignores the stored best config and overwrites it\n",
    "\n",
    "#### I have the best model stored, but I have changed the best params\n",
    "Set forceRetrain=True in construct_and_train_model_with_config so it ignores the stored model and overwrites with a new one\n",
    "\n",
    "#### I just want to train one model with specified params\n",
    "You can always call the above functions separately, just make sure you define valid inputs (note that the keys are not named the same as in the config), and give a run name. The run name will define in what directories will the output be saved. It defaults to \"best\", so on default the ouput is saved in best_configs, best_models, best_model_results, but given e.g. \"lstm\" they would be saved to lstm_configs ... (directories created in the script). Any function that needs to access some stored file can take runName as argument, and all default to \"best\". As an example, training a simple LSTM encoder without sweeping and SentEval evaluation, with custom params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "custom_params = {\n",
    "    \"learning rate\": 0.0001,\n",
    "    \"lr_stopping\": 1e-06,\n",
    "    \"lr_decrease_factor\": 7,\n",
    "    \"number of layers\": 1,\n",
    "    \"number of neurons per layer\": 256\n",
    "}\n",
    "\n",
    "runName = \"custom_lstm_run\"\n",
    "\n",
    "trained_model = construct_and_train_model_with_config(LSTMEncoder, data, custom_params, metadata, runName=runName)\n",
    "trained_model_results = testModel(trained_model, data)\n",
    "save_model_and_res(trained_model, trained_model_results, runName = runName)\n",
    "\n",
    "## we can also call the result printing with the runName\n",
    "printResults([\"LSTM\"], resultType = \"SNLI\", runName = runName)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results and models that are reported in this notebook (so the ones under the \"best\" folders) are actually not the output of parameter sweeping, but using the same parameters as in the Conneau paper for easier comparison.\\\n",
    "\\\n",
    "There are two differences from the original paper's setup:\n",
    "* I used **dropout** of 0.5 at both the encoder and the classifier (original did not report any dropout)\n",
    "* I used **Adam optimizer**. For that, the (starting) learning rate and the stopping had to be reduced, to 0.001 and 1e-06.\n",
    "\n",
    "If we compare the results to the ones reported in the paper we can see two things:\n",
    "* The SNLI results are comparable, slightly better here\n",
    "* The transfer results are noticably worse\n",
    "\n",
    "This gives the idea that we are not overfitting the data, but we **are overfitting the task**. My intuition was that it might be due to the Adam optimizer, as the other difference, the dropout is a regularization that should not give task overfitting. To test that I ran a version with SGD optimizer (change optimizer in the trainFunctions), the corresponding files can be found in the sgd_... folders.\n",
    "\n",
    "Let us look at how the results compare:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results with Adam\n",
      "\n",
      "| Model         |   dev accuracy |   test accuracy:  |   transfer macro |   transfer micro |\n",
      "|---------------+----------------+-------------------+------------------+------------------|\n",
      "| LSTM          |        82.67   |           82.7618 |          75.63   |          77.8306 |\n",
      "| BiLSTM        |        82.3872 |           82.4269 |          75.5136 |          77.792  |\n",
      "| Pooled BiLSTM |        84.3397 |           84.3141 |          77.479  |          79.1903 |\n",
      "\n",
      "Results with SGD\n",
      "\n",
      "| Model         |   dev accuracy |   test accuracy:  |   transfer macro |   transfer micro |\n",
      "|---------------+----------------+-------------------+------------------+------------------|\n",
      "| LSTM          |        80.5349 |           80.3064 |          77.3029 |          79.1279 |\n",
      "| BiLSTM        |        80.1843 |           80.3571 |          77.3843 |          78.9623 |\n",
      "| Pooled BiLSTM |        80.222  |           79.86   |          79.1748 |          80.8795 |\n"
     ]
    }
   ],
   "source": [
    "encoderNames = [\"LSTM\", \"BiLSTM\", \"Pooled BiLSTM\"]  \n",
    "print(\"\\nResults with Adam\\n\")\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\")\n",
    "print(\"\\nResults with SGD\\n\")\n",
    "printResults(encoderNames, resultType = \"SNLI+transfer\", runName = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results with Adam\n",
      "\n",
      "| Model         |    MR |    CR |   MPQA |   SUBJ |   SST2 |   TREC | MRPC        |   SICKEntailment | STS14     |\n",
      "|---------------+-------+-------+--------+--------+--------+--------+-------------+------------------+-----------|\n",
      "| LSTM          | 68.54 | 75.23 |  83.48 |  82.1  |  71.94 |   65.6 | 69.51/78.77 |            82.52 | 0.53/0.51 |\n",
      "| BiLSTM        | 68.19 | 75.1  |  83.67 |  82.54 |  70.02 |   66.2 | 70.78/80.75 |            82.06 | 0.54/0.51 |\n",
      "| Pooled BiLSTM | 73.21 | 80.05 |  85.26 |  88.57 |  77.38 |   81.6 | 72.75/81.18 |            83.8  | 0.64/0.61 |\n",
      "\n",
      "Results with SGD\n",
      "\n",
      "| Model         |    MR |    CR |   MPQA |   SUBJ |   SST2 |   TREC | MRPC        |   SICKEntailment | STS14     |\n",
      "|---------------+-------+-------+--------+--------+--------+--------+-------------+------------------+-----------|\n",
      "| LSTM          | 70.22 | 75.28 |  84.2  |  84.27 |  74.24 |   70.2 | 72.93/81.49 |            82.71 | 0.57/0.56 |\n",
      "| BiLSTM        | 69.22 | 75.79 |  84.4  |  83.33 |  75.07 |   71.8 | 71.83/80.13 |            82.65 | 0.57/0.56 |\n",
      "| Pooled BiLSTM | 78.6  | 84.72 |  86.93 |  90.75 |  78.58 |   75.6 | 73.51/82.14 |            84.11 | 0.64/0.63 |\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResults with Adam\\n\")\n",
    "printResults(encoderNames, resultType = \"SentEval\")\n",
    "print(\"\\nResults with SGD\\n\")\n",
    "printResults(encoderNames, resultType = \"SentEval\", runName=\"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('dl': conda)",
   "language": "python",
   "name": "python37364bitdlconda4a4146cd19534b21b826a4d8caea02c6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
